{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\p36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.1.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import reuters\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8982\n",
      "2246\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 245, 273, 207, 156, 53, 74, 160, 26, 14, 46, 296, 26, 39, 74, 2979, 3554, 14, 46, 4689, 4329, 86, 61, 3499, 4795, 14, 61, 451, 4329, 17, 12]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = reuters.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "# Note that our indices were offset by 3\n",
    "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
    "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_newswire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "# vectorized training data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "\n",
    "# vectorized test data\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(labels, dimension=46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "\n",
    "one_hot_train_labels = to_one_hot(train_labels)\n",
    "one_hot_test_labels = to_one_hot(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or by keras built-in method\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "one_hot_train_labels = to_categorical(train_labels)\n",
    "one_hot_test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 4s 462us/step - loss: 2.4997 - acc: 0.4899 - val_loss: 1.6813 - val_acc: 0.6480\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 1s 93us/step - loss: 1.3915 - acc: 0.7038 - val_loss: 1.2790 - val_acc: 0.7190\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 1.0487 - acc: 0.7699 - val_loss: 1.1181 - val_acc: 0.7610\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 93us/step - loss: 0.8246 - acc: 0.8282 - val_loss: 1.0217 - val_acc: 0.7760\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s 87us/step - loss: 0.6599 - acc: 0.8637 - val_loss: 0.9688 - val_acc: 0.7970\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 1s 93us/step - loss: 0.5254 - acc: 0.8931 - val_loss: 0.9200 - val_acc: 0.8090\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 1s 89us/step - loss: 0.4291 - acc: 0.9118 - val_loss: 0.9108 - val_acc: 0.8030\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 1s 92us/step - loss: 0.3497 - acc: 0.9277 - val_loss: 0.8937 - val_acc: 0.8150\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 0.2893 - acc: 0.9386 - val_loss: 0.9128 - val_acc: 0.8090\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 89us/step - loss: 0.2450 - acc: 0.9453 - val_loss: 0.9114 - val_acc: 0.8100\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 0.2102 - acc: 0.9481 - val_loss: 0.9482 - val_acc: 0.8130\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 88us/step - loss: 0.1878 - acc: 0.9528 - val_loss: 0.9613 - val_acc: 0.8040\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 0.1658 - acc: 0.9529 - val_loss: 0.9926 - val_acc: 0.8010\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 1s 89us/step - loss: 0.1532 - acc: 0.9550 - val_loss: 0.9772 - val_acc: 0.8050\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 1s 87us/step - loss: 0.1456 - acc: 0.9550 - val_loss: 1.0188 - val_acc: 0.7990\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 1s 87us/step - loss: 0.1326 - acc: 0.9554 - val_loss: 1.0390 - val_acc: 0.8000\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 0.1255 - acc: 0.9554 - val_loss: 1.0423 - val_acc: 0.7970\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 1s 89us/step - loss: 0.1170 - acc: 0.9560 - val_loss: 1.0367 - val_acc: 0.8150\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 1s 88us/step - loss: 0.1157 - acc: 0.9573 - val_loss: 1.0290 - val_acc: 0.8080\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 0.1136 - acc: 0.9582 - val_loss: 1.0496 - val_acc: 0.8030\n"
     ]
    }
   ],
   "source": [
    "# Training the network for 20 epochs\n",
    "history = model.fit(partial_x_train,\n",
    "                   partial_y_train,\n",
    "                   epochs=20,\n",
    "                   batch_size=512,\n",
    "                   validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRBL+P4edhN/QJGWjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x256a3273898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x256a3273710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # clear figure\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/8\n",
      "7982/7982 [==============================] - 1s 131us/step - loss: 2.5398 - acc: 0.5226 - val_loss: 1.6733 - val_acc: 0.6570\n",
      "Epoch 2/8\n",
      "7982/7982 [==============================] - 1s 89us/step - loss: 1.3712 - acc: 0.7121 - val_loss: 1.2758 - val_acc: 0.7210\n",
      "Epoch 3/8\n",
      "7982/7982 [==============================] - 1s 91us/step - loss: 1.0136 - acc: 0.7781 - val_loss: 1.1303 - val_acc: 0.7530\n",
      "Epoch 4/8\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 0.7976 - acc: 0.8251 - val_loss: 1.0539 - val_acc: 0.7590\n",
      "Epoch 5/8\n",
      "7982/7982 [==============================] - 1s 92us/step - loss: 0.6393 - acc: 0.8624 - val_loss: 0.9754 - val_acc: 0.7920\n",
      "Epoch 6/8\n",
      "7982/7982 [==============================] - 1s 88us/step - loss: 0.5124 - acc: 0.8921 - val_loss: 0.9102 - val_acc: 0.8140\n",
      "Epoch 7/8\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 0.4124 - acc: 0.9139 - val_loss: 0.8932 - val_acc: 0.8210\n",
      "Epoch 8/8\n",
      "7982/7982 [==============================] - 1s 92us/step - loss: 0.3355 - acc: 0.9290 - val_loss: 0.8733 - val_acc: 0.8260\n",
      "2246/2246 [==============================] - 0s 161us/step\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# Training the network for 20 epochs\n",
    "history = model.fit(partial_x_train,\n",
    "                   partial_y_train,\n",
    "                   epochs=8,\n",
    "                   batch_size=512,\n",
    "                   validation_data = (x_val, y_val))\n",
    "\n",
    "results = model.evaluate(x_test, one_hot_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9844293263164463, 0.7836153161175423]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19679430097951914"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "test_labels_copy = copy.copy(test_labels)\n",
    "np.random.shuffle(test_labels_copy)\n",
    "float(np.sum(np.array(test_labels) == np.array(test_labels_copy))) / len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating predictions on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000004"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 1s 127us/step - loss: 3.4021 - acc: 0.2959 - val_loss: 2.9755 - val_acc: 0.5710\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 1s 92us/step - loss: 2.6915 - acc: 0.5735 - val_loss: 2.4263 - val_acc: 0.5930\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 2.1771 - acc: 0.6045 - val_loss: 2.0529 - val_acc: 0.5920\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 95us/step - loss: 1.8147 - acc: 0.6188 - val_loss: 1.7789 - val_acc: 0.6140\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s 91us/step - loss: 1.5728 - acc: 0.6278 - val_loss: 1.6205 - val_acc: 0.6240\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 1.4038 - acc: 0.6612 - val_loss: 1.5234 - val_acc: 0.6860\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 1s 92us/step - loss: 1.2762 - acc: 0.7184 - val_loss: 1.4582 - val_acc: 0.6860\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 1.1791 - acc: 0.7309 - val_loss: 1.4225 - val_acc: 0.6980\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 91us/step - loss: 1.1044 - acc: 0.7405 - val_loss: 1.3873 - val_acc: 0.7000\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 93us/step - loss: 1.0431 - acc: 0.7478 - val_loss: 1.3673 - val_acc: 0.7010\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s 89us/step - loss: 0.9943 - acc: 0.7537 - val_loss: 1.3669 - val_acc: 0.7040\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 91us/step - loss: 0.9532 - acc: 0.7596 - val_loss: 1.3693 - val_acc: 0.7030\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 1s 94us/step - loss: 0.9126 - acc: 0.7638 - val_loss: 1.3790 - val_acc: 0.6960\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 1s 91us/step - loss: 0.8793 - acc: 0.7658 - val_loss: 1.3730 - val_acc: 0.7020\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 1s 92us/step - loss: 0.8449 - acc: 0.7754 - val_loss: 1.3902 - val_acc: 0.7000\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 1s 91us/step - loss: 0.8160 - acc: 0.7834 - val_loss: 1.4030 - val_acc: 0.7000\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 0.7848 - acc: 0.7910 - val_loss: 1.3975 - val_acc: 0.7040\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 1s 92us/step - loss: 0.7594 - acc: 0.7973 - val_loss: 1.4327 - val_acc: 0.7010\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 1s 92us/step - loss: 0.7336 - acc: 0.8013 - val_loss: 1.4709 - val_acc: 0.6950\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 1s 92us/step - loss: 0.7089 - acc: 0.8038 - val_loss: 1.4471 - val_acc: 0.7070\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x256d0bb8b00>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(4, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# Training the network for 20 epochs\n",
    "model.fit(partial_x_train,\n",
    "       partial_y_train,\n",
    "       epochs=20,\n",
    "       batch_size=512,\n",
    "       validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 1s 130us/step - loss: 2.2326 - acc: 0.5570 - val_loss: 1.4157 - val_acc: 0.6920\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 1s 91us/step - loss: 1.1268 - acc: 0.7577 - val_loss: 1.1093 - val_acc: 0.7550\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s 95us/step - loss: 0.7927 - acc: 0.8325 - val_loss: 0.9844 - val_acc: 0.7980\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 95us/step - loss: 0.5614 - acc: 0.8845 - val_loss: 0.9127 - val_acc: 0.8050\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s 95us/step - loss: 0.4161 - acc: 0.9127 - val_loss: 0.9436 - val_acc: 0.7880\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 1s 94us/step - loss: 0.3185 - acc: 0.9328 - val_loss: 0.8559 - val_acc: 0.8230\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 1s 92us/step - loss: 0.2444 - acc: 0.9437 - val_loss: 0.9105 - val_acc: 0.8160\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 1s 94us/step - loss: 0.2058 - acc: 0.9491 - val_loss: 0.8883 - val_acc: 0.8280\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 93us/step - loss: 0.1875 - acc: 0.9513 - val_loss: 0.9927 - val_acc: 0.7920\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 93us/step - loss: 0.1617 - acc: 0.9536 - val_loss: 0.9958 - val_acc: 0.7950\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s 91us/step - loss: 0.1491 - acc: 0.9551 - val_loss: 0.9715 - val_acc: 0.8090\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 94us/step - loss: 0.1434 - acc: 0.9562 - val_loss: 0.9772 - val_acc: 0.8130\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 1s 92us/step - loss: 0.1307 - acc: 0.9565 - val_loss: 1.0053 - val_acc: 0.8000\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 1s 92us/step - loss: 0.1262 - acc: 0.9572 - val_loss: 1.0187 - val_acc: 0.8060\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 1s 94us/step - loss: 0.1223 - acc: 0.9575 - val_loss: 1.0145 - val_acc: 0.8050\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 1s 92us/step - loss: 0.1136 - acc: 0.9578 - val_loss: 1.1203 - val_acc: 0.7900\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 1s 94us/step - loss: 0.1133 - acc: 0.9612 - val_loss: 1.0801 - val_acc: 0.7900\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 1s 92us/step - loss: 0.1173 - acc: 0.9558 - val_loss: 1.0349 - val_acc: 0.8100\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 1s 91us/step - loss: 0.1083 - acc: 0.9582 - val_loss: 1.0966 - val_acc: 0.7870\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 1s 93us/step - loss: 0.1120 - acc: 0.9558 - val_loss: 1.1151 - val_acc: 0.7920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25706711748>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(128, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# Training the network for 20 epochs\n",
    "model.fit(partial_x_train,\n",
    "       partial_y_train,\n",
    "       epochs=20,\n",
    "       batch_size=512,\n",
    "       validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 1s 128us/step - loss: 2.8416 - acc: 0.5287 - val_loss: 1.9858 - val_acc: 0.6250\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 1s 89us/step - loss: 1.6520 - acc: 0.6798 - val_loss: 1.4847 - val_acc: 0.6770\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s 95us/step - loss: 1.2684 - acc: 0.7281 - val_loss: 1.2797 - val_acc: 0.7150\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 94us/step - loss: 1.0507 - acc: 0.7729 - val_loss: 1.1728 - val_acc: 0.7510\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s 89us/step - loss: 0.8882 - acc: 0.8113 - val_loss: 1.0986 - val_acc: 0.7690\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 1s 91us/step - loss: 0.7542 - acc: 0.8368 - val_loss: 1.0266 - val_acc: 0.7770\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 0.6394 - acc: 0.8597 - val_loss: 0.9862 - val_acc: 0.7930\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 0.5408 - acc: 0.8796 - val_loss: 0.9686 - val_acc: 0.7980\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 91us/step - loss: 0.4595 - acc: 0.8999 - val_loss: 0.9320 - val_acc: 0.7970\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 91us/step - loss: 0.3929 - acc: 0.9152 - val_loss: 0.9254 - val_acc: 0.8050\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 0.3330 - acc: 0.9291 - val_loss: 0.9331 - val_acc: 0.8060\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 88us/step - loss: 0.2925 - acc: 0.9361 - val_loss: 0.9380 - val_acc: 0.8040\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 1s 91us/step - loss: 0.2524 - acc: 0.9416 - val_loss: 0.9292 - val_acc: 0.8150\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 1s 93us/step - loss: 0.2225 - acc: 0.9458 - val_loss: 0.9529 - val_acc: 0.8140\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 1s 93us/step - loss: 0.2017 - acc: 0.9484 - val_loss: 0.9312 - val_acc: 0.8170\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 1s 88us/step - loss: 0.1783 - acc: 0.9510 - val_loss: 0.9529 - val_acc: 0.8120\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 1s 89us/step - loss: 0.1646 - acc: 0.9534 - val_loss: 0.9479 - val_acc: 0.8160\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 1s 88us/step - loss: 0.1530 - acc: 0.9529 - val_loss: 0.9772 - val_acc: 0.8140\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 0.1421 - acc: 0.9548 - val_loss: 0.9808 - val_acc: 0.8120\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 1s 88us/step - loss: 0.1371 - acc: 0.9553 - val_loss: 1.0257 - val_acc: 0.8060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2582d62db00>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# Training the network for 20 epochs\n",
    "model.fit(partial_x_train,\n",
    "       partial_y_train,\n",
    "       epochs=20,\n",
    "       batch_size=512,\n",
    "       validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 1s 135us/step - loss: 2.6184 - acc: 0.4607 - val_loss: 1.7149 - val_acc: 0.6150\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 1s 92us/step - loss: 1.4662 - acc: 0.6833 - val_loss: 1.3544 - val_acc: 0.6880\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s 89us/step - loss: 1.1377 - acc: 0.7400 - val_loss: 1.2025 - val_acc: 0.7320\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 88us/step - loss: 0.8902 - acc: 0.8029 - val_loss: 1.0986 - val_acc: 0.7700\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s 91us/step - loss: 0.7241 - acc: 0.8365 - val_loss: 1.0327 - val_acc: 0.7750\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 0.5602 - acc: 0.8730 - val_loss: 1.0594 - val_acc: 0.7650\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 1s 92us/step - loss: 0.4617 - acc: 0.8961 - val_loss: 1.0001 - val_acc: 0.7950\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 1s 91us/step - loss: 0.3703 - acc: 0.9182 - val_loss: 0.9828 - val_acc: 0.7920\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 94us/step - loss: 0.3012 - acc: 0.9322 - val_loss: 0.9888 - val_acc: 0.7970\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 91us/step - loss: 0.2662 - acc: 0.9412 - val_loss: 0.9928 - val_acc: 0.8050\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s 94us/step - loss: 0.2131 - acc: 0.9470 - val_loss: 1.0305 - val_acc: 0.8080\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 91us/step - loss: 0.2015 - acc: 0.9503 - val_loss: 1.0639 - val_acc: 0.8000\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 1s 91us/step - loss: 0.1716 - acc: 0.9534 - val_loss: 1.0813 - val_acc: 0.8030\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 1s 92us/step - loss: 0.1648 - acc: 0.9529 - val_loss: 1.1214 - val_acc: 0.7970\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 1s 94us/step - loss: 0.1435 - acc: 0.9549 - val_loss: 1.0773 - val_acc: 0.8050\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 1s 93us/step - loss: 0.1427 - acc: 0.9570 - val_loss: 1.1457 - val_acc: 0.8020\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 1s 94us/step - loss: 0.1359 - acc: 0.9578 - val_loss: 1.1636 - val_acc: 0.7990\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 1s 92us/step - loss: 0.1302 - acc: 0.9565 - val_loss: 1.1306 - val_acc: 0.7980\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 0.1261 - acc: 0.9575 - val_loss: 1.2899 - val_acc: 0.7850\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 1s 94us/step - loss: 0.1210 - acc: 0.9584 - val_loss: 1.3829 - val_acc: 0.7570\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25835efb898>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# Training the network for 20 epochs\n",
    "model.fit(partial_x_train,\n",
    "       partial_y_train,\n",
    "       epochs=20,\n",
    "       batch_size=512,\n",
    "       validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 1s 138us/step - loss: 2.7383 - acc: 0.4118 - val_loss: 1.8922 - val_acc: 0.5740\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 1s 93us/step - loss: 1.6048 - acc: 0.6703 - val_loss: 1.4417 - val_acc: 0.6920\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 1.2453 - acc: 0.7286 - val_loss: 1.2748 - val_acc: 0.7120\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 88us/step - loss: 1.0329 - acc: 0.7660 - val_loss: 1.1567 - val_acc: 0.7520\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s 89us/step - loss: 0.8632 - acc: 0.8079 - val_loss: 1.0974 - val_acc: 0.7540\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 1s 89us/step - loss: 0.7237 - acc: 0.8385 - val_loss: 1.0362 - val_acc: 0.7660\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 0.6067 - acc: 0.8675 - val_loss: 1.0457 - val_acc: 0.7750\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 0.5058 - acc: 0.8882 - val_loss: 0.9823 - val_acc: 0.7970\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 91us/step - loss: 0.4230 - acc: 0.9069 - val_loss: 0.9906 - val_acc: 0.8050\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 0.3612 - acc: 0.9207 - val_loss: 0.9638 - val_acc: 0.8020\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s 89us/step - loss: 0.3096 - acc: 0.9307 - val_loss: 0.9900 - val_acc: 0.7980\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 0.2623 - acc: 0.9367 - val_loss: 1.0174 - val_acc: 0.7990\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 1s 92us/step - loss: 0.2337 - acc: 0.9414 - val_loss: 1.0332 - val_acc: 0.7970\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 1s 89us/step - loss: 0.2039 - acc: 0.9483 - val_loss: 1.0329 - val_acc: 0.8010\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 1s 92us/step - loss: 0.1890 - acc: 0.9523 - val_loss: 1.0763 - val_acc: 0.7910\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 1s 91us/step - loss: 0.1697 - acc: 0.9539 - val_loss: 1.1475 - val_acc: 0.7800\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 1s 94us/step - loss: 0.1551 - acc: 0.9558 - val_loss: 1.2928 - val_acc: 0.7530\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 1s 91us/step - loss: 0.1512 - acc: 0.9573 - val_loss: 1.1744 - val_acc: 0.7860\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 1s 89us/step - loss: 0.1397 - acc: 0.9558 - val_loss: 1.2867 - val_acc: 0.7720\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 1s 92us/step - loss: 0.1373 - acc: 0.9580 - val_loss: 1.1436 - val_acc: 0.7960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2583614bf28>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# Training the network for 20 epochs\n",
    "model.fit(partial_x_train,\n",
    "       partial_y_train,\n",
    "       epochs=20,\n",
    "       batch_size=512,\n",
    "       validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 1s 157us/step - loss: 2.7081 - acc: 0.4798 - val_loss: 1.7380 - val_acc: 0.6180\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 1s 89us/step - loss: 1.5002 - acc: 0.6808 - val_loss: 1.3892 - val_acc: 0.6840\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 1.1868 - acc: 0.7316 - val_loss: 1.2276 - val_acc: 0.7110\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 91us/step - loss: 0.9873 - acc: 0.7615 - val_loss: 1.1299 - val_acc: 0.7400\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 0.8186 - acc: 0.8104 - val_loss: 1.0750 - val_acc: 0.7530\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 1s 89us/step - loss: 0.6754 - acc: 0.8419 - val_loss: 1.0151 - val_acc: 0.7730\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 0.5563 - acc: 0.8662 - val_loss: 1.0491 - val_acc: 0.7570\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 0.4567 - acc: 0.8871 - val_loss: 1.0268 - val_acc: 0.7770\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 91us/step - loss: 0.3753 - acc: 0.9087 - val_loss: 1.0179 - val_acc: 0.7960\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 89us/step - loss: 0.3193 - acc: 0.9265 - val_loss: 0.9735 - val_acc: 0.8030\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s 89us/step - loss: 0.2680 - acc: 0.9384 - val_loss: 1.0101 - val_acc: 0.8040\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 0.2335 - acc: 0.9448 - val_loss: 1.0926 - val_acc: 0.7850\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 1s 89us/step - loss: 0.2058 - acc: 0.9486 - val_loss: 1.0301 - val_acc: 0.8100\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 1s 91us/step - loss: 0.1893 - acc: 0.9506 - val_loss: 1.0447 - val_acc: 0.8090\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 1s 92us/step - loss: 0.1604 - acc: 0.9554 - val_loss: 1.0574 - val_acc: 0.8060\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 1s 89us/step - loss: 0.1533 - acc: 0.9578 - val_loss: 1.1028 - val_acc: 0.8130\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 1s 89us/step - loss: 0.1475 - acc: 0.9582 - val_loss: 1.1209 - val_acc: 0.7950\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 1s 88us/step - loss: 0.1413 - acc: 0.9578 - val_loss: 1.1739 - val_acc: 0.7990\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 1s 91us/step - loss: 0.1385 - acc: 0.9569 - val_loss: 1.1013 - val_acc: 0.8110\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 1s 91us/step - loss: 0.1317 - acc: 0.9554 - val_loss: 1.3226 - val_acc: 0.7770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x258380a4ef0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# Training the network for 20 epochs\n",
    "model.fit(partial_x_train,\n",
    "       partial_y_train,\n",
    "       epochs=20,\n",
    "       batch_size=512,\n",
    "       validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/15\n",
      "7982/7982 [==============================] - 1s 137us/step - loss: 2.8648 - acc: 0.4618 - val_loss: 2.0413 - val_acc: 0.5790\n",
      "Epoch 2/15\n",
      "7982/7982 [==============================] - 1s 91us/step - loss: 1.6833 - acc: 0.6536 - val_loss: 1.5163 - val_acc: 0.6680\n",
      "Epoch 3/15\n",
      "7982/7982 [==============================] - 1s 91us/step - loss: 1.2838 - acc: 0.7234 - val_loss: 1.3009 - val_acc: 0.7180\n",
      "Epoch 4/15\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 1.0646 - acc: 0.7735 - val_loss: 1.1775 - val_acc: 0.7430\n",
      "Epoch 5/15\n",
      "7982/7982 [==============================] - 1s 88us/step - loss: 0.8975 - acc: 0.8047 - val_loss: 1.1153 - val_acc: 0.7550\n",
      "Epoch 6/15\n",
      "7982/7982 [==============================] - 1s 88us/step - loss: 0.7645 - acc: 0.8296 - val_loss: 1.0480 - val_acc: 0.7680\n",
      "Epoch 7/15\n",
      "7982/7982 [==============================] - 1s 89us/step - loss: 0.6511 - acc: 0.8520 - val_loss: 0.9869 - val_acc: 0.7980\n",
      "Epoch 8/15\n",
      "7982/7982 [==============================] - 1s 92us/step - loss: 0.5551 - acc: 0.8738 - val_loss: 0.9750 - val_acc: 0.7870\n",
      "Epoch 9/15\n",
      "7982/7982 [==============================] - 1s 91us/step - loss: 0.4703 - acc: 0.8968 - val_loss: 0.9296 - val_acc: 0.8000\n",
      "Epoch 10/15\n",
      "7982/7982 [==============================] - 1s 88us/step - loss: 0.4049 - acc: 0.9110 - val_loss: 0.9484 - val_acc: 0.7980\n",
      "Epoch 11/15\n",
      "7982/7982 [==============================] - 1s 92us/step - loss: 0.3409 - acc: 0.9247 - val_loss: 0.9099 - val_acc: 0.8120\n",
      "Epoch 12/15\n",
      "7982/7982 [==============================] - 1s 89us/step - loss: 0.2936 - acc: 0.9354 - val_loss: 0.9241 - val_acc: 0.8140\n",
      "Epoch 13/15\n",
      "7982/7982 [==============================] - 1s 89us/step - loss: 0.2549 - acc: 0.9401 - val_loss: 0.9366 - val_acc: 0.8120\n",
      "Epoch 14/15\n",
      "7982/7982 [==============================] - 1s 88us/step - loss: 0.2248 - acc: 0.9453 - val_loss: 0.9480 - val_acc: 0.8070\n",
      "Epoch 15/15\n",
      "7982/7982 [==============================] - 1s 91us/step - loss: 0.1986 - acc: 0.9495 - val_loss: 0.9490 - val_acc: 0.8100\n",
      "2246/2246 [==============================] - 0s 136us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0602128011036005, 0.7840605521456854]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# Training the network for 20 epochs\n",
    "model.fit(partial_x_train,\n",
    "       partial_y_train,\n",
    "       epochs=15,\n",
    "       batch_size=512,\n",
    "       validation_data = (x_val, y_val))\n",
    "\n",
    "results = model.evaluate(x_test, one_hot_test_labels)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
